<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A Robotic Assistance Platform for Self-Feeding">
  <meta name="keywords" content="Robot-Assisted Feeding, Eye-tracking, Spinal Cord Injury">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CHMS-RAF</title>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-FV4ZJ9PVSV"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FV4ZJ9PVSV');
  </script>

  <script>
    function updateSingleVideo() {
      var demo = document.getElementById("single-menu-demos").value;
      var task = document.getElementById("single-menu-tasks").value;

    //   console.log("single", demo, task)

      var video = document.getElementById("multi-task-result-video");
      video.src = "media/results/qpred/" + 
                  demo +
                  "-" +
                  task +
                  ".mp4"
      video.playbackRate = 1.0;
      video.play();
    }

    function updateQpredVideo() {
      var task = document.getElementById("single-menu-qpred").value;

    //   console.log("qpred", task)

      var video = document.getElementById("q-pred-video");
      video.src = "media/results/qpred/" + 
                  task + 
                  ".mp4"
      video.playbackRate = 1.0;
      video.play();
    }

  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body onload="updateSingleVideo(); updateQpredVideo();">

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" target="_blank" href="https://engineering.csuohio.edu/research/center-for-human-machine-systems">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" target="_blank" href="https://github.com/chms-raf/eye-hand-coordination">
            Eye-Hand Coordination
          </a>
          <a class="navbar-item" target="_blank" href="https://github.com/chms-raf/preference-adaptation">
            Preference Adaptation
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">A Robotic Assistance Platform <br> for Self-Feeding</h1>
          <h3 class="title is-4 conference-authors"><a target="_blank" href="https://www.rehabweek.org">RehabWeek 2022</a></h3>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank" href="https://jschultz299.github.io">John R. Schultz</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://facultyprofile.csuohio.edu/csufacultyprofile/detail.cfm?FacultyID=A_SLIFKIN">Andrew B. Slifkin</a><sup>2</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://facultyprofile.csuohio.edu/csufacultyprofile/detail.cfm?FacultyID=e_schearer">Eric M. Schearer</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Cleveland State University (Mechanical Engineering),</span>
            <span class="author-block"><sup>2</sup>Cleveland State University (Psychology)</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" href="paper/chms-raf-icorr2022.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

            <!-- Video Link. -->
            <span class="link-block">
              <a target="_blank" href="https://youtu.be/AmBzfEcXVCc"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-youtube"></i>
                </span>
                <span>Video</span>
              </a>
            </span>

            <!-- Code Link. -->
            <span class="link-block">
              <a target="_blank" href="https://github.com/chms-raf/raf-v2"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
            </span>

            </div>
          </div>
<!--           <br>
          <br> -->


        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
            <img src="media/intro/rafv2-system.png" width="90%" alt="Robot-Assisted Feeding system version 2." />
          </br>
        </div>
        <br>
        <h2 class="subtitle has-text-centered">
        We developed a <b>robotic assistance platform</b> to enable people with spinal cord injuries <br>
        to feed themselves using their <b>eye movements</b>.
        </h2>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay muted loop height="100%">
            <source src="media/intro/1-selection.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay muted loop height="100%">
            <source src="media/intro/2-simulation.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay muted loop height="100%">
            <source src="media/intro/3-eat-pretzel.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay muted loop height="100%">
            <source src="media/intro/4-object-detection.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay muted loop height="100%">
            <source src="media/intro/5-mouth-detection.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay muted loop height="100%">
            <source src="media/intro/6-angle-detection.mp4"
                    type="video/mp4">
          </video>
        </div>
        </div>
      </div>
    </div>
  </div>
</section>
<h2 class="subtitle has-text-centered">
</br>
  The system consits primarily of a <b>Kinova Gen3 robot</b> attached to a powered wheelchair.<br><br>
  Capabilities include detecting, localizing, acquiring, and delivering food items <br>
  to a person with a <b>spinal cord injury</b>. The person communicates their intention to the system<br>
  using their <b>eye movements</b> to interact with a custom <b>graphical user interface</b>.
</h2>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Eating and drinking is an essential part of everyday life. And yet, there are many people in the world today
            who rely on others to feed them. In this work, we present a prototype robot-assisted self-feeding system for individuals
            with movement disorders. The system is capable of perceiving, localizing, grasping, and delivering non-compliant food items
            to an individual. 
          </p>
          <p>
            We trained an object recognition network to detect specific food items, and we compute the grasp
            pose for each item. Human input is obtained through an interface consisting of an eye-tracker and a display screen. The
            human selects options on the monitor with their eye and head movements and triggers responses with mouth movements. 
          </p>
          <p>
            We performed a pilot study with five able-bodied participants and five participants with a spinal cord injuries (SCI) to evaluate
            the performance of our prototype system. Participants selected food items with their eye movements, which were then delivered
            by the robot.
          </p>
          <p>
            We observed an average overall feeding success rate of 91.6% and an average overall task time of 34.2 ± 4.0
            seconds per food item. The SCI participants gave scores of 83.1 and 12.1 on the System Usability Scale and NASA Task
            Load Index, respectively. We also conducted a custom, post-study interview to gather participant feedback to drive future
            design decisions.
          </p>
        </div>
      </div>
    </div>
    <br>
    <br>
    <!--/ Abstract. -->

  </div>

  <!-- Paper video. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-two-thirds">
      <h2 class="title is-3">Video</h2>
      <div class="publication-video">
        <iframe src="https://www.youtube.com/embed/AmBzfEcXVCc"
                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div>
    </div>
  </div>

</section>

<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">


    <!-- Animation. -->
    <div class="rows is-centered ">
      <div class="row is-full-width">
        <h2 class="title is-3"><span class="dperact">Robotic Assistance Platform</span></h2>

        <!-- Interpolating. -->
        <h3 class="title is-4"><span class="dperact">RAFv1</span> - Overview</h3>
        <div class="content has-text-justified">
        <!-- <br> -->
        </div>
        <p>
          The overall goal of our research is to enable people with disabilities to live productive and healthy lifestyles.
          The goal of this work was to develop a robotic platform for helping people with spinal cord injuries
          to perform tasks of everyday living, with a primary emphasis on feeding.
          <br>
          <br>
          We began by developing a prototype robot-assisted feeding system. The system consisted primarily of a Baxter research robot 
          positioned across the table from the user. The person sits in their powered wheelchair, equipped with a monitor and a commercial 
          eye tracker. A depth camera is mounted to the wrist of the robot to handle perception, and the user communicates their intention 
          to the system using their eye movements. This initial protoype was our 'Version 1' robot-assisted feeding device, or '<span class="dperact">RAFv1</span>'.   
        </p>
        </br>
        </br>
        <img src="media/figures/system-overview.png" class="interpolation-image" 
         alt="Interpolate start reference image." />
        </br>
        </br>
        <!-- Interpolating. -->
        <h3 class="title is-4"><span class="dperact">RAFv1</span> - Detection and Localization</h3>
        <div class="content has-text-justified">
        <!-- <br> -->
        </div>
        <p>
          To detect food items on a plate, we trained a mask R-CNN object detection network to recognize symmetric, non-compliant food items 
          such as carrots, pretzels, and celery. We used AprilTag fiducial markers to register the coordinate frame for the scene and computed 
          the grasp pose for food item acquisition.
        </p>
        </br>
        <video id="object-detection-grasp-pose"
             muted
             autoplay
             loop
             width="100%">
        <source src="media/figures/object-detection-grasp-pose.mp4" 
                type="video/mp4">
        </video>
        </br>
        </br>

        <h3 class="title is-4">Custom Graphical User Interface</h3>
          <p class="justify">
            <img src="media/figures/gui-cursor2.gif" class="interpolation-image" width="25%" align="right"
                 style="margin:0% 4% "
                 alt="Interpolate start reference image." />
             We developed a custom graphical user interface (GUI) which facilitates bi-directional communication between 
             the user and the system. The GUI primarily displays the output of the depth camera attached to the robot 
             overlaid with detections from the mask R-CNN inference. The person uses their eyes to control a dwell-time cursor 
             to make selections on the screen. When the cursor enters the bounding box of a selectable food item, it will highlight 
             to indicate its interactivity. If the cursor remains in the box for 1.5s, the item is selected. The primary benefit 
             of this setup is that users are in control over the most important decisions of the feeding task: what to eat and when to eat it.
          </p>
        <br/>
        <br/>

        <h3 class="title is-4">Mouth Detection</h3>
          <p class="justify">
            <img src="media/figures/mouth-detection.gif" class="interpolation-image" width="25%" align="left"
                 style="margin:0% 4% 0% 0%"
                 alt="Interpolate start reference image." />
                 To detect the user's facial keypoints, we used the <a target="_blank" href="https://github.com/1adrianb/face-alignment">face-alignment</a> python package. 
                 This package detects 60 facial landmarks, 12 of which belong to the mouth. We fit an ellipse 
                 to those 12 mouth landmarks by minimizing a least-squares cost function. By comparing the ratio 
                 of the principal axes of the ellipse to a threshold, we can get a good indication of whether the 
                 mouth is open or closed. By opening their mouth, the user indicates when they are ready to eat. 
                 This gives the person additional control over the pacing of the feeding task.
          </p>
        <br/>
        <br/>
        <br/>

        <h3 class="title is-4"><span class="dperact">RAFv1</span> - Pilot Study</h3>
        <div class="content has-text-justified">
        <!-- <br> -->
        </div>
        <p>
            In this work, we conducted a pilot study in which we presented the system to individuals 
            with spinal cord injuries and other mobility impairments. People were able to try out the 
            system, practice feeding themselves, and discover critical failure modes. The goal was to 
            develop a baseline for system performance as well as to capture human experience when using 
            the device. Five able-bodied individuals (2 male, 3 female; 25 ± 4.3 years old) and five 
            people with spinal cord injury or other movement disorders (3 male, 2 female, 48 ± 13.3 
            years old) participated in the study.
            <br>
            <br>
            To evaluate system performance, we measured both success rates and task times for each of the 
            study phases. After data collection, we had participants complete the NASA Task Load Index 
            (TLX) and the System Usability Scale (SUS). To better understand the needs of the disability 
            community, we gathered qualitative feedback from participants in the form of a custom, 
            semi-structured, post-study interview.
        </p>
        <br/>
        <br/>
        

        <!--/ Re-rendering. -->

        <h2 class="title is-3">Results</h2>

        <h3 class="title is-4">Demonstrations</h3>

        <div class="columns">
          <div class="column has-text-centered">
            <h3 class="title is-5">Demonstrations</h3>

            System: 
            <div class="select is-small">
              <select id="single-menu-demos" onchange="updateSingleVideo()">
              <option value="RAFv1">RAFv1</option>
              <option value="RAFv2" selected="selected">RAFv2</option>
              </select>
            </div>
            , Task: 
            <div class="select is-small">     
              <select id="single-menu-tasks" onchange="updateSingleVideo()">
              <option value="Grasp" selected="selected">Grasp</option>
              <option value="Sip">Sip</option>
              <option value="Visual-Servoing">Visual Servoing</option>
              </select>
            </div>
            <br/>
            <br/>

            <video id="multi-task-result-video"
                   muted
                   autoplay
                   loop
                   width="99%">
              <source src="media/results/qpred/RAFv2-Grasp.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
        </br>
        </br>

        <h3 class="title is-4">Demonstrations</h3>

        <div class="columns">
          <div class="column has-text-centered">
            <h3 class="title is-5">Demonstrations</h3>

            View Demo for   
            <div class="select is-small is-rounded">     
              <select id="single-menu-qpred" onchange="updateQpredVideo()">
              <option value="RAFv2-Grasp">"RAFv2 - Eat Pretzel"</option>
              <option value="RAFv2-Sip">"RAFv2 - Sip Drink"</option>
              <option value="RAFv2-Visual-Servoing">"RAFv2 - Visual Servoing"</option>
              <option value="RAFv1-Grasp">"RAFv1 - Eat Pretzel"</option>
              <option value="RAFv1-Sip">"RAFv1 - Sip Drink"</option>
              </select>
            </div>
            <br/>
            <br/>
            <video id="q-pred-video"
                muted
                autoplay
                loop
                width="99%">
            <source src="media/results/qpred/RAFv2-Grasp.mp4"
                    type="video/mp4">
            </video>
          </div>

      </div>
    </div>
  </div>
</section>

<br>
<br>
<!-- <section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">
      <h2 class="title is-3">Emergent Properties</h2>

      <h3 class="title is-4">Tracking Objects</h3>
      A selected example of tracking an unseen hand sanitizer instance with an agent that was trained on a single object with 5 "press the handsan" demos. Since <span class="dperact">PerAct</span> focuses on actions, it doesn't need a complete representation of the bottle, and only has to predict <b><i>where</i> to press</b> the sanitizer.

      <video id="tracking-objects"
             muted
             autoplay
             loop
             width="99%">
        <source src="media/results/animations/handsan_tracking_v2.mp4" 
                type="video/mp4">
      </video>

    </div>

  </div>

</section> -->


<section class="section" id="BibTeX">
  <div class="container is-max-widescreen content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{schultz2022proof,
        title={Proof-of-Concept: A Hands-Free Interface for Robot-Assisted Self-Feeding},
        author={Schultz, John R and Slifkin, Andrew B and Yu, Hongkai and Schearer, Eric M},
        booktitle={2022 International Conference on Rehabilitation Robotics (ICORR)},
        pages={1--6},
        year={2022},
        organization={IEEE}
      }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> made by the amazing <a href="https://keunhong.com/">Keunhong Park</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
