<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A Robotic Assistance Platform for Self-Feeding">
  <meta name="keywords" content="Robot-Assisted Feeding, Eye-tracking, Spinal Cord Injury">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CHMS-RAF</title>

  <script async src="https://www.googletagmanager.com/gtag/js?id=G-FV4ZJ9PVSV"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-FV4ZJ9PVSV');
  </script>

  <script>
    function updateSingleVideo() {
      var demo = document.getElementById("single-menu-demos").value;
      var task = document.getElementById("single-menu-tasks").value;
      var inst = document.getElementById("single-menu-instances").value;

      console.log("single", demo, task, inst)

      var video = document.getElementById("multi-task-result-video");
      video.src = "media/results/sim_rollouts/" + 
                  "n" +
                  demo +
                  "-" +
                  task +
                  "-" +
                  inst +
                  ".mp4"
      video.playbackRate = 1.75;
      video.play();
    }

    function updateQpredVideo() {
      var task = document.getElementById("single-menu-qpred").value;

      console.log("qpred", task)

      var video = document.getElementById("q-pred-video");
      video.src = "media/results/qpred/" + 
                  task + 
                  ".mp4"
      video.playbackRate = 1.75;
      video.play();
    }

  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body onload="updateSingleVideo(); updateQpredVideo();">

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" target="_blank" href="https://engineering.csuohio.edu/research/center-for-human-machine-systems">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" target="_blank" href="https://github.com/chms-raf/eye-hand-coordination">
            Eye-Hand Coordination
          </a>
          <a class="navbar-item" target="_blank" href="https://github.com/chms-raf/preference-adaptation">
            Preference Adaptation
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">A Robotic Assistance Platform <br> for Self-Feeding</h1>
          <h3 class="title is-4 conference-authors"><a target="_blank" href="https://www.rehabweek.org">RehabWeek 2022</a></h3>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank" href="https://jschultz299.github.io">John R. Schultz</a><sup>1</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://facultyprofile.csuohio.edu/csufacultyprofile/detail.cfm?FacultyID=A_SLIFKIN">Andrew B. Slifkin</a><sup>2</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://facultyprofile.csuohio.edu/csufacultyprofile/detail.cfm?FacultyID=e_schearer">Eric M. Schearer</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Cleveland State University (Mechanical Engineering),</span>
            <span class="author-block"><sup>2</sup>Cleveland State University (Psychology)</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" href="paper/chms-raf-icorr2022.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

            <!-- Video Link. -->
            <span class="link-block">
              <a target="_blank" href="https://youtu.be/AmBzfEcXVCc"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-youtube"></i>
                </span>
                <span>Video</span>
              </a>
            </span>

            <!-- Code Link. -->
            <span class="link-block">
              <a target="_blank" href="https://github.com/chms-raf/raf-v2"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
            </span>

            </div>
          </div>
<!--           <br>
          <br> -->


        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="container">
        <div class="columns is-vcentered  is-centered">
            <img src="media/intro/rafv2-system.png" width="90%" alt="Robot-Assisted Feeding system version 2." />
          </br>
        </div>
        <br>
        <h2 class="subtitle has-text-centered">
        We developed a <b>robotic assistance platform</b> to enable people with spinal cord injuries <br>
        to feed themselves using their <b>eye movements</b>.
        </h2>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay muted loop height="100%">
            <source src="media/intro/1-selection.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay muted loop height="100%">
            <source src="media/intro/2-simulation.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay muted loop height="100%">
            <source src="media/intro/3-eat-pretzel.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay muted loop height="100%">
            <source src="media/intro/4-object-detection.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay muted loop height="100%">
            <source src="media/intro/5-mouth-detection.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay muted loop height="100%">
            <source src="media/intro/6-angle-detection.mp4"
                    type="video/mp4">
          </video>
        </div>
        </div>
      </div>
    </div>
  </div>
</section>
<h2 class="subtitle has-text-centered">
</br>
  The system consits primarily of a <b>Kinova Gen3 robot</b> attached to a powered wheelchair.<br><br>
  Capabilities include detecting, localizing, acquiring, and delivering food items <br>
  to a person with a <b>spinal cord injury</b>. The person communicates their intention to the system<br>
  using their <b>eye movements</b> to interact with a custom <b>graphical user interface</b>.
</h2>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Eating and drinking is an essential part of everyday life. And yet, there are many people in the world today
            who rely on others to feed them. In this work, we present a prototype robot-assisted self-feeding system for individuals
            with movement disorders. The system is capable of perceiving, localizing, grasping, and delivering non-compliant food items
            to an individual. 
          </p>
          <p>
            We trained an object recognition network to detect specific food items, and we compute the grasp
            pose for each item. Human input is obtained through an interface consisting of an eye-tracker and a display screen. The
            human selects options on the monitor with their eye and head movements and triggers responses with mouth movements. 
          </p>
          <p>
            We performed a pilot study with five able-bodied participants and five participants with a spinal cord injuries (SCI) to evaluate
            the performance of our prototype system. Participants selected food items with their eye movements, which were then delivered
            by the robot.
          </p>
          <p>
            We observed an average overall feeding success rate of 91.6% and an average overall task time of 34.2 ± 4.0
            seconds per food item. The SCI participants gave scores of 83.1 and 12.1 on the System Usability Scale and NASA Task
            Load Index, respectively. We also conducted a custom, post-study interview to gather participant feedback to drive future
            design decisions.
          </p>
        </div>
      </div>
    </div>
    <br>
    <br>
    <!--/ Abstract. -->

  </div>

  <!-- Paper video. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-two-thirds">
      <h2 class="title is-3">Video</h2>
      <div class="publication-video">
        <iframe src="https://www.youtube.com/embed/AmBzfEcXVCc"
                frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div>
    </div>
  </div>

</section>

<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">


    <!-- Animation. -->
    <div class="rows is-centered ">
      <div class="row is-full-width">
        <h2 class="title is-3"><span class="dperact">PerAct</span></h2>

        <!-- Interpolating. -->
        <h3 class="title is-4">A Transformer for Detecting Actions</h3>
        <div class="content has-text-justified">
        <!-- <br> -->
        </div>
        <p>
          <span class="dperact">PerAct</span> is a language-conditioned behavior-cloning agent trained with supervised learning to <i>detect actions</i>. Instead of using object-detectors, instance-segmentors, or pose-estimators to represent a scene and then learning a policy, <span class="dperact">PerAct</span> directly learns <b>perceptual representations of actions</b> conditioned on language goals. This <a target="_blank" href="https://en.wikipedia.org/wiki/Ecological_psychology">action-centric approach</a> with a unified observation and action space makes <span class="dperact">PerAct</span> applicable to a broad range of tasks involving articulated objects, deformable objects, granular media, and even some non-prehensile interactions with tools.  
        </p>
        </br>
        </br>
        <img src="media/figures/arch.png" class="interpolation-image" 
         alt="Interpolate start reference image." />
        </br>
        </br>
          <p>
              <span class="dperact">PerAct</span> takes as input a language goal and a voxel grid reconstructed from RGB-D sensors. The voxels are split into 3D patches (like <a target="_blank" href="https://arxiv.org/abs/2010.11929">vision transformers</a> split images into 2D patches), and the language goal is encoded with a pre-trained language model. The language and voxel features are appended together as a sequence and encoded with a <a target=”_blank” href="https://www.deepmind.com/blog/building-architectures-that-can-handle-the-worlds-data">PerceiverIO Transformer</a> to learn per-voxel features. These features are then reshaped with linear layers to predict a discretized translation, rotation, gripper open, and collision avoidance action, which can be executed with a motion-planner. 
              <!-- This action is executed with a motion-planner after which the new observation is used to predict the next discrete action in an observe-act loop until termination.  -->
              Overall, the voxelized observation and action space provides a strong structural prior for efficiently learning 6-DoF polices. Checkout our <a target="_blank" href="https://colab.research.google.com/drive/1HAqemP4cE81SQ6QO1-N85j5bF4C0qLs0?usp=sharing">Colab Tutorial</a> for an annotated guide on implemententing <span class="dperact">PerAct</span> and training it from scratch on a single GPU.
          </p>
        </br>
        </br>
        <h3 class="title is-4">Encoding High-Dimensional Input</h3>
          <p class="justify">
            <img src="media/figures/perceiver.png" class="interpolation-image" width="480" align="right"
                 style="margin:0% 4% "
                 alt="Interpolate start reference image." />
             The input grid is 100&times;100&times;100 = 1 million voxels. After extracting 5&times;5&times;5 patches, the input is 20&times;20&times;20 = 8000 embeddings long. Despite this long sequence, Perceiver uses a small set of latent vectors to encode the input. These latent vectors are randomly initialized and trained end-to-end. This approach decouples the depth of the Transformer self-attention layers from the dimensionality of the input space, which allows us train <span class="dperact">PerAct</span> on very large input voxel grids. Perceiver has been deployed in several domains like <a target="_blank" href="https://www.deepmind.com/publications/perceiver-ar-general-purpose-long-context-autoregressive-generation">long-context auto-regressive generation</a>, <a target="_blank" href="https://arxiv.org/abs/2204.14198">vision-language models for few-shot learning</a>, <a target="_blank" href="https://arxiv.org/abs/2107.14795">image and audio classification, and optical flow prediction.</a>
          </p>
        <br/>
        <br/>

        <!--/ Re-rendering. -->

        <h2 class="title is-3">Results</h2>

        <h3 class="title is-4">Simulation Results</h3>

        <div class="columns">
          <div class="column has-text-centered">
            <h3 class="title is-5">One Multi-Task Transformer</h3>

            Trained with
            <div class="select is-small">
              <select id="single-menu-demos" onchange="updateSingleVideo()">
              <option value="10">10</option>
              <option value="100" selected="selected">100</option>
              </select>
            </div>
            demos per task, evaluated on 
            <div class="select is-small">     
              <select id="single-menu-tasks" onchange="updateSingleVideo()">
              <option value="open_drawer" selected="selected">open drawer</option>
              <option value="slide_block">slide block</option>
              <option value="sweep_to_dustpan">sweep to dustpan</option>
              <option value="meat_off_grill">meat off grill</option>
              <option value="turn_tap">turn tap</option>
              <option value="put_in_drawer">put in drawer</option>
              <option value="close_jar">close jar</option>
              <option value="drag_stick">drag stick</option>
              <option value="stack_blocks">stack blocks</option>
              <option value="screw_bulb">screw bulb</option>
              <option value="put_in_safe">put in safe</option>
              <option value="place_wine">place wine</option>
              <option value="put_in_cupboard">put in cupboard</option>
              <option value="sort_shape">sort shape</option>
              <option value="push_buttons">push buttons</option>
              <option value="insert_peg">insert peg</option>
              <option value="stack_cups">stack cups</option>
              <option value="place_cups">place cups</option>
              </select>
            </div>
            episode
            <div class="select is-small">
              <select id="single-menu-instances" onchange="updateSingleVideo()">
              <option value="s1">01</option>
              <option value="s2" selected="selected">02</option>
              <option value="s3">03</option>
              <option value="s4">04</option>
              <option value="s5">05</option>
              </select>
            </div>
            <br/>
            <br/>

            <video id="multi-task-result-video"
                   muted
                   autoplay
                   loop
                   width="100%">
              <source src="media/results/sim_rollouts/n10-open_drawer-s2.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
        </br>
        </br>

        <h3 class="title is-4">Action Predictions</h3>

        <div class="columns">
          <div class="column has-text-centered">
            <h3 class="title is-5">Q-Prediction Examples</h3>

            Visualize predictions for   
            <div class="select is-small is-rounded">     
              <select id="single-menu-qpred" onchange="updateQpredVideo()">
              <option value="tomato" selected="selected">"put the tomatoes in the top bin"</option>
              <option value="stick">"hit the green ball with the stick"</option>
              <option value="handsan">"press the hand san"</option>
              <option value="tape">"put the tape in the top drawer"</option>
              </select>
            </div>
          </div>

      </div>
    </div>
  </div>
</section>

<video id="q-pred-video"
       muted
       autoplay
       loop
       width="100%">
  <source src="media/results/qpred/tomato.mp4"
          type="video/mp4">
</video>

<br>
<br>
<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">
      <h2 class="title is-3">Emergent Properties</h2>

      <h3 class="title is-4">Tracking Objects</h3>
      A selected example of tracking an unseen hand sanitizer instance with an agent that was trained on a single object with 5 "press the handsan" demos. Since <span class="dperact">PerAct</span> focuses on actions, it doesn't need a complete representation of the bottle, and only has to predict <b><i>where</i> to press</b> the sanitizer.

      <video id="tracking-objects"
             muted
             autoplay
             loop
             width="99%">
        <source src="media/results/animations/handsan_tracking_v2.mp4" 
                type="video/mp4">
      </video>

    </div>

  </div>

</section>


<section class="section" id="BibTeX">
  <div class="container is-max-widescreen content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{schultz2022proof,
        title={Proof-of-Concept: A Hands-Free Interface for Robot-Assisted Self-Feeding},
        author={Schultz, John R and Slifkin, Andrew B and Yu, Hongkai and Schearer, Eric M},
        booktitle={2022 International Conference on Rehabilitation Robotics (ICORR)},
        pages={1--6},
        year={2022},
        organization={IEEE}
      }</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> made by the amazing <a href="https://keunhong.com/">Keunhong Park</a>. 
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>


</body>
</html>
